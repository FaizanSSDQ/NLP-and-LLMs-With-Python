{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a274fc85-fd33-4bd8-b684-c9110007bc71",
   "metadata": {
    "tags": []
   },
   "source": [
    "# **NLP-Based Text Generation Using N-Gram Models**\n",
    "\n",
    "## **Finally, You're Here!** ðŸš€  \n",
    "\n",
    "This project is part of an ongoing series of explorations in **Natural Language Processing (NLP)** and **Language Modeling** that I have been working on. Here, I dive into the fundamentals of **text generation** using statistical approaches, starting with simple **frequency-based probability distributions** and advancing to **N-Gram models** (unigram, bigram, and trigram).  \n",
    "\n",
    "Throughout this notebook, I have explained the core concepts, detailed the working of the code, and demonstrated how these models generate text. Whether you're new to NLP or looking to refine your understanding, this journey will provide valuable insights into the mechanics of language modeling.  \n",
    "\n",
    "Letâ€™s get started! ðŸš€  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72498044-23f0-4993-b184-226a75ef121c",
   "metadata": {},
   "source": [
    "**Note:** I have provided a basic theoretical explanation for different concepts. Some whcih is my own written and some is gathered from different sources. The ultimate idea is to keep the understanding and explanation of the project smooth and fluffy\n",
    "\n",
    "I have avoided writing any comments in the code cells to keep cells neat, but have provided explanation of the codes in the preceding cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96945480-bf8b-4a9b-9998-ac5e24240498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Introduction**\n",
    "\n",
    "This project serves as an introduction to the field of language modeling, focusing on creating a text generator tailored for composing 90s rap songs. I have utilized histogram N-gram models, implemented through the Natural Language Toolkit (NLTK). This approach allows me to construct revealing histograms, shedding light on nuanced cadences of word frequencies and distributions.\n",
    "\n",
    "These initial steps lay the foundation for understanding the intricacies of linguistic patterns.  will first try to generate or predict a word based on simple probability distribution. After analyzing the challenges we would face, we will move to improved solution, that is N-Grams model. Progressing forward, I have implmented bigram and trigram models as well.\n",
    "\n",
    "Throughout this journey, we are going to explore various training strategies and embrace fundamental Natural Language Processing (NLP) tasks, including tokenization and sequence analysis. As you traverse this enriching path, you will gain profound insights into the art of generating text, culminating in the ability to craft 90s rap lyrics that resonate with the era's unique style and rhythm.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-GPXX0WSVEN/song%20%281%29.png\" alt=\"Image Description\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0c68e9-de6c-47be-94c1-3aa778906030",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "            <li><a href=\"#Defining-helper-functions\">Defining helper functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "        <li><a href=\"#Language-modeling\">Language modeling</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Tokenization\">Tokenization</a></li>\n",
    "            <li><a href=\"#Unigram-model\">Unigram model</a></li>\n",
    "            <li><a href=\"#Bigram-model\">Bigram model</a></li>\n",
    "            <li><a href=\"#Trigram-Model\">Trigram model</a></li>\n",
    "        </ol>\n",
    "    </li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8829f-612e-421f-8aae-c7785e262ed1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a9ab0e-c948-4e28-95b8-0034d5f950fa",
   "metadata": {},
   "source": [
    "For this project, you will use the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b0835-d353-47eb-87dc-55b8cfb13c03",
   "metadata": {},
   "source": [
    "### Installing required libraries\n",
    "\n",
    "You will need to install these libraries using the code cell below.\n",
    "\n",
    "<h2 style=\"color:red;\">After installing the libraries below please RESTART THE KERNEL and run all cells.</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6300509c-7811-49b3-9150-2d769c8afc13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!mamba install -y nltk\n",
    "!pip install torchtext -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0bfff-0cec-4fc9-8d0a-3c832aba5910",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Importing required libraries\n",
    "\n",
    "_I have imported libraries in steps, not all in one go as there were oome version and compatibility issues. But for you it is recommended that you import all required libraries in one place (here):_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba680951-af98-4653-bb12-fff38a23ee49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import time\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28b8fee5-14e2-4b33-97c6-c70de8f01b34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import string\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e62b191-028d-4a8e-a6f1-591b6125d731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09af7ed0-27f2-4f52-a34d-684e3750b9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn.manifold as tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c23f8e6-1caf-4d2c-9d6a-76db1f2014e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def warn(*args , **kwargs):\n",
    "    pass\n",
    "\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.simplefilter(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82025234-65cc-40ff-9a1b-74c718aed38a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'module'>\n"
     ]
    }
   ],
   "source": [
    "print(type(tsne))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c35734-e29f-473d-a771-c0e9a8625f2f",
   "metadata": {},
   "source": [
    "### Defining helper functions\n",
    "\n",
    "Remove all non-word characters (everything except numbers and letters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39954873-5289-48e5-a8b7-5029f06b1f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_string(s):\n",
    "    s = re.sub(r\"[^\\s\\w]\" , \"\" , s)\n",
    "    s = re.sub(r\"[\\s+]\" , \"\" , s) \n",
    "    s = re.sub(r\"[\\d]\" , \"\" , s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cf49411-c17c-493b-a281-5a9f4d58291b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_s = preprocess_string(\"Hey ! , what an amazing 69 posit_ion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40256a6b-d98b-427d-ad0a-e354095545fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heywhatanamazingposit_ion\n"
     ]
    }
   ],
   "source": [
    "print(new_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eac41b-d5d3-4b19-bd21-82440d9e7314",
   "metadata": {},
   "source": [
    "## Language modeling\n",
    "\n",
    "Language modeling is a foundational concept within the field of natural language processing (NLP) and artificial intelligence. It involves the prediction of the likelihood of a sequence of words within a given language. This method is statistical in nature and seeks to capture the patterns, structures, and relationships that exist between words in a given text corpus.\n",
    "\n",
    "At its essence, a language model strives to comprehend the probabilities associated with sequences of words. This comprehension can be leveraged for a multitude of NLP tasks, including but not limited to text generation, machine translation, speech recognition, sentiment analysis, and more.\n",
    "\n",
    "Let's consider the following song lyrics to determine if you can generate similar output using a given word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81827f00-a812-4272-93b3-0a9209960127",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "song= \"\"\"\n",
    "We are no strangers to love\n",
    "You know the rules and so do I\n",
    "A full commitments what Im thinking of\n",
    "You wouldnt get this from any other guy\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "And if you ask me how Im feeling\n",
    "Dont tell me youre too blind to see\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Weve known each other for so long\n",
    "Your hearts been aching but youre too shy to say it\n",
    "Inside we both know whats been going on\n",
    "We know the game and were gonna play it\n",
    "I just wanna tell you how Im feeling\n",
    "Gotta make you understand\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\n",
    "Never gonna give you up\n",
    "Never gonna let you down\n",
    "Never gonna run around and desert you\n",
    "Never gonna make you cry\n",
    "Never gonna say goodbye\n",
    "Never gonna tell a lie and hurt you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba0bf7e-ddc2-4a80-bea1-6fcff21cc7d9",
   "metadata": {},
   "source": [
    "### Natural Language Toolkit (NLTK)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95848ddb-87fd-4e54-bb8b-b5e8966bcdee",
   "metadata": {},
   "source": [
    "NLTK is indeed a widely-used open-source library in Python that is specifically designed for various natural language processing (NLP) tasks. It provides a comprehensive set of tools, resources, and algorithms that aid in the analysis and manipulation of human language data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08980a-650b-4e08-a41c-9e222f4fcc38",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Tokenization, a fundamental concept within the realm of natural language processing (NLP), involves the intricate process of breaking down a body of text into discrete units known as tokens. These tokens can encompass words, phrases, sentences, or even individual characters, adapting based on the desired level of granularity for analysis. For the purpose of this project, you will focus on Word Tokenization, a prevalent technique. This technique treats each word in the text as an independent entity. Words, typically separated by spaces or punctuation marks, serve as the tokens in this approach. It's important to note that Word Tokenization exhibits versatile characteristics, including capitalization, symbols, and punctuation marks.\n",
    "\n",
    "To achieve the goal, you will utilize the```word_tokenize```function. During this process, you will remove punctuation, symbols, and capital letters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c2c617-4b85-4fa0-b0e4-4def3f4aab57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "def preprocess(words):\n",
    "    tokens=word_tokenize(words)\n",
    "    tokens=[preprocess_string(w)   for w in tokens]\n",
    "    return [w.lower()  for w in tokens if len(w)!=0 or not(w in string.punctuation) ]\n",
    "\n",
    "tokens=preprocess(song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e1fbf-325a-4510-8f75-13f2e836c33c",
   "metadata": {},
   "source": [
    "The outcome is a collection of tokens, wherein each element of the```tokens```pertains to the lyrics of the song, arranged in sequential order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "004be21c-aeed-4331-a1b1-dc97ec35a520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8431f0-82f4-48cf-8b23-5e9174dafc7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'are',\n",
       " 'no',\n",
       " 'strangers',\n",
       " 'to',\n",
       " 'love',\n",
       " 'you',\n",
       " 'know',\n",
       " 'the',\n",
       " 'rules',\n",
       " 'and',\n",
       " 'so',\n",
       " 'do',\n",
       " 'i',\n",
       " 'a']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac4582-c312-4574-988d-57aa4115366b",
   "metadata": {},
   "source": [
    "The frequency distribution of words in a sentence represents how often each word appears in that particular sentence. It provides a count of the occurrences of individual words, allowing you to understand which words are more common or frequent within the given sentence. Let's work with the following toy example:\n",
    "\n",
    "```Text```: **I like dogs and I kinda like cats**\n",
    "\n",
    "```Tokens```: **[I like, dogs, and, I, kinda, like, cats]**\n",
    "\n",
    "The function```Count```will tally the occurrences of words in the input text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79172be-a40e-4c1e-a8c8-558efb2b622c",
   "metadata": {},
   "source": [
    "$Count(\"I\")=2$\n",
    "\n",
    "$Count(\"like\")= 2$\n",
    "\n",
    "$Count(\"dogs\")=1$\n",
    "\n",
    "$Count(\"and\")=1$\n",
    "\n",
    "$Count(\"kinda\")=1$\n",
    "\n",
    "$Count(\"cats\")=1$\n",
    "\n",
    "$\\text{Total words} =8$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9120eba1-a7a8-4dcf-8fa0-64c4bd1968a6",
   "metadata": {},
   "source": [
    "Utilize```NLTK's FreqDist```to transform a frequency distribution of words. The outcome is a Python dictionary where the keys correspond to words, and the values indicate the frequency of each word's appearance. Please consider the provided example below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0878b720-3fc9-43ec-881b-71bd9f6f3586",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'na': 40, 'gon': 38, 'you': 37, 'never': 36, 'and': 16, 'tell': 9, 'make': 8, 'say': 8, 'a': 7, 'give': 6, ...})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist = nltk.FreqDist(tokens)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed149017-cd71-46c4-b4c2-07db758fd846",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "key_list = list(fdist.keys())[0:10]\n",
    "val_list = list(fdist.values())[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75a231-120b-46f8-8ff9-00579b9b2c54",
   "metadata": {},
   "source": [
    "Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2725abd0-cdc4-465c-b4d1-ab036204ba38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(key_list , val_list)\n",
    "plt.xlabel(\"Words\")\n",
    "plt.ylabel(\"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b1e92ff-c882-41b2-9a88-91496649883b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "385"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = sum(fdist.values())\n",
    "C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83559bd-60c2-4b2e-8e55-4382392c8c67",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Unigram model\n",
    "\n",
    "An unigram model is a simple type of language model that considers each word in a sequence independently, without taking into account the previous words. In other words, it models the probability of each word occurring in the text, regardless of what came before it. Unigram models can be seen as a special case of n-gram models, where n is 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3556f-c907-48cc-a713-0e4845afce2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "You can think that text follows patterns, and probabilities are used to measure how likely a sequence of words is. In a unigram model, each word is considered independent and doesn't rely on others. Let's calculate the probability of **'I like tiramisu but I love cheesecake more'**.\n",
    "\n",
    "$  P(\\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I\"})}{\\text{Total words}}=\\frac{2}{8} = 0.250  $\n",
    "\n",
    "$  P(\\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"tiramisu\"}) = \\frac{\\text{Count}(\\text{\"tiramisu\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"but\"}) = \\frac{\\text{Count}(\\text{\"but\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I\"})}{\\text{Total words}}=\\frac{2}{8} = 0.250  $\n",
    "\n",
    "$  P(\\text{\"love\"}) = \\frac{\\text{Count}(\\text{\"love\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"cheesecake\"}) = \\frac{\\text{Count}(\\text{\"cheesecake\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$  P(\\text{\"more\"}) = \\frac{\\text{Count}(\\text{\"more\"})}{\\text{Total words}}=\\frac{1}{8} = 0.125  $\n",
    "\n",
    "$P(\\text{\"I\"}, \\text{\"like\"}, \\text{\"tiramisu\"}, \\text{\"but\"}, \\text{\"I\"}, \\text{\"love\"}, \\text{\"cheesecake\"}, \\text{\"more\"}) = P(\\text{\"I\"}) \\cdot P(\\text{\"like\"}) \\cdot P(\\text{\"tiramisu\"}) \\cdot P(\\text{\"but\"}) \\cdot P(\\text{\"I\"}) \\cdot P(\\text{\"love\"}) \\cdot P(\\text{\"cheesecake\"}) \\cdot P(\\text{\"more\"}) = 0.250 \\times 0.125 \\times 0.125 \\times 0.125 \\times 0.250 \\times 0.125 \\times 0.125 \\times 0.125$\n",
    "\n",
    "In general, language models boil down to predicting a sequence of length $t$: $P(W_t, W_{t-1}, ..., W_0)$. In this eight-word sequence, you have:\n",
    "\n",
    "$P(W_7=\\text{\"more\"}, W_6=\\text{\"cheesecake\"}, W_5=\\text{\"love\"}, W_4=\\text{\"I\"}, W_3=\\text{\"but\"}, W_2=\\text{\"tiramisu\"}, W_1=\\text{\"like\"}, W_0=\\text{\"I\"})$\n",
    "\n",
    "The subscript serves as a positional indicator in the sequence and does not impact the nature of $P(\\bullet)$. When formally expressing the sequence, the last word is positioned at the leftmost side, gradually descending as you move through the sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf0e1dc-8a91-42ab-b3e3-137e1dd550e4",
   "metadata": {},
   "source": [
    "Using NLTK you can normalize the frequency values by dividing them by the total count of each word to get a probability function. Now you will find the probability of each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ad867908-8451-45a5-a13a-ee57bbf8411e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025974025974025974"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist[\"strangers\"]/C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26350982-d275-48a0-97a8-53c0b1cf35f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Finding the probabilities of all tokens\n",
    "values = []\n",
    "\n",
    "for x in fdist.keys():\n",
    "    values.append(fdist[f\"{x}\"]/C)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b572c6b-f7e3-41f4-acd4-fb813837dab6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1038961038961039\n"
     ]
    }
   ],
   "source": [
    "print(max(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4d2502c7-1536-4667-93dd-dbff6fcba89f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "385\n"
     ]
    }
   ],
   "source": [
    "print(type(tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a002b-7486-4b5d-9d68-9cbbdb45a547",
   "metadata": {},
   "source": [
    "Next, find each individual word by converting the tokens to a set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb8da1b5-ec76-444a-a6d9-77bf6de6e2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocabulary = set(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39129012-39eb-445e-ab58-884534cdc874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'any', 'on', 'love', 'so', 'and', 'up', 'we', 'each', 'a', 'ask', 'shy', 'give', 'inside', 'been', 'if', 'to', 'gon', 'weve', 'tell', 'commitments', 'whats', 'never', 'youre', 'na', 'this', 'thinking', 'wan', 'known', 'how', 'hearts', 'other', 'it', 'just', 'let', 'me', 'know', 'of', 'full', 'i', 'are', 'understand', 'strangers', 'dont', 'around', 'get', 'both', 'play', 'lie', 'what', 'guy', 'do', 'blind', 'hurt', 'the', 'long', 'no', 'run', 'from', 'you', 'got', 'desert', 'too', 'aching', 'down', 'say', 'im', 'make', 'were', 'but', 'see', 'game', 'ta', 'cry', 'goodbye', 'wouldnt', 'rules', 'feeling', 'going', 'for', 'your'}\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "print(vocabulary)\n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "215afa36-99a5-4d7e-8045-e54321eec44a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'no', 'strangers', 'to', 'love', 'you', 'know', 'the', 'rules', 'and', 'so', 'do', 'i', 'a', 'full', 'commitments', 'what', 'im', 'thinking', 'of', 'you', 'wouldnt', 'get', 'this', 'from', 'any', 'other', 'guy', 'i', 'just', 'wan', 'na', 'tell', 'you', 'how', 'im', 'feeling', 'got', 'ta', 'make', 'you', 'understand', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you', 'weve', 'known', 'each', 'other', 'for', 'so', 'long', 'your', 'hearts', 'been', 'aching', 'but', 'youre', 'too', 'shy', 'to', 'say', 'it', 'inside', 'we', 'both', 'know', 'whats', 'been', 'going', 'on', 'we', 'know', 'the', 'game', 'and', 'were', 'gon', 'na', 'play', 'it', 'and', 'if', 'you', 'ask', 'me', 'how', 'im', 'feeling', 'dont', 'tell', 'me', 'youre', 'too', 'blind', 'to', 'see', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you', 'weve', 'known', 'each', 'other', 'for', 'so', 'long', 'your', 'hearts', 'been', 'aching', 'but', 'youre', 'too', 'shy', 'to', 'say', 'it', 'inside', 'we', 'both', 'know', 'whats', 'been', 'going', 'on', 'we', 'know', 'the', 'game', 'and', 'were', 'gon', 'na', 'play', 'it', 'i', 'just', 'wan', 'na', 'tell', 'you', 'how', 'im', 'feeling', 'got', 'ta', 'make', 'you', 'understand', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you', 'never', 'gon', 'na', 'give', 'you', 'up', 'never', 'gon', 'na', 'let', 'you', 'down', 'never', 'gon', 'na', 'run', 'around', 'and', 'desert', 'you', 'never', 'gon', 'na', 'make', 'you', 'cry', 'never', 'gon', 'na', 'say', 'goodbye', 'never', 'gon', 'na', 'tell', 'a', 'lie', 'and', 'hurt', 'you']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10068e4c-aa56-425d-a5e7-ac470f147fa0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### How unigram model predicts the next likely word\n",
    "\n",
    "Let's consider a scenario from the above example **'I like tiramisu but I love cheesecake more'** where the unigram model is asked to predict the next word following the sequence **'I like'**.\n",
    "\n",
    "If the highest probability among all words is **\"I\"** with a probability  0.25, then according to the model, the most likely next word after **'I like'** would be **'I'**. However, this prediction doesn't make sense at all. This highlights a significant limitation of the unigram modelâ€”it lacks context, and its predictions are entirely dependent on the word with the highest probability \"I\" in this case \n",
    "\n",
    "Even if multiple words have the same highest probabilities, it will randomly choose any one word out of all the options.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2573032c-65a7-480f-be8e-5569345c45e1",
   "metadata": {},
   "source": [
    "### Bigram model\n",
    "\n",
    "Bigrams represent pairs of consecutive words in the given phrase, i.e., $(w_{t-1},w_t)$. Consider the following words from your example: \"I like dogs and I kinda like cats.\"\n",
    "\n",
    "The correct sequence of bigrams is:\n",
    "\n",
    "$(I, like)$\n",
    "\n",
    "$(like, dogs)$\n",
    "\n",
    "$(dogs, and)$\n",
    "\n",
    "$(and, I)$\n",
    "\n",
    "$(I, kinda)$\n",
    "\n",
    "$(kinda, like)$\n",
    "\n",
    "$(like, cats)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175ea324-4fab-4056-b065-8a549cca3d65",
   "metadata": {},
   "source": [
    "**2-Gram models**: Bigram models use conditional probability. The probability of a word depends only on the previous word, i.e., the conditional probability $(W_{t}, W_{t-1})$ is used to predict the likelihood of word $(W_t)$ following word $W_{t-1}$ in a sequence. You can calculate the conditional probability for a bigram model using the following steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b31110-0cfb-42f3-a87b-971919732c15",
   "metadata": {},
   "source": [
    "Perform the bigram word count for each bigram: $Count(W_{t-1}, W_{t})$\n",
    "\n",
    "$Count(\\text{I, like}) = 1$\n",
    "\n",
    "$Count(\\text{like, dogs}) = 1$\n",
    "\n",
    "$Count(\\text{dogs, and}) = 1$\n",
    "\n",
    "$Count(\\text{and, I}) = 1$\n",
    "\n",
    "$Count(\\text{I, kinda}) = 1$\n",
    "\n",
    "$Count(\\text{kinda, like}) = 1$\n",
    "\n",
    "$Count(\\text{like, cats}) = 1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82286e2a-aa43-44f3-a747-b2c0f6b44afa",
   "metadata": {},
   "source": [
    "Now, let's calculate the conditional probability for each bigram in the form of $P(w_{t} | w_{t-1})$, where $w_{t-1}$ is the **context**, and the context size is one.\n",
    "\n",
    "$P(\\text{\"like\"} | \\text{\"I\"}) = \\frac{\\text{Count}(\\text{\"I, like\"})}{\\text{Total count of \"I\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "$P(\\text{\"dogs\"} | \\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like, dogs\"})}{\\text{Total count of \"like\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "$:$\n",
    "\n",
    "$P(\\text{\"like\"} | \\text{\"kinda\"}) = \\frac{\\text{Count}(\\text{\"kinda, like\"})}{\\text{Total count of \"kinda\"}} = \\frac{1}{1} = 1$\n",
    "\n",
    "$P(\\text{\"cats\"} | \\text{\"like\"}) = \\frac{\\text{Count}(\\text{\"like, cats\"})}{\\text{Total count of \"like\"}} = \\frac{1}{2} = 0.5$\n",
    "\n",
    "These probabilities represent the likelihood of encountering the second word in a bigram, given the presence of the first word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f6ecd-ad69-412b-a7d9-0fb216a36071",
   "metadata": {},
   "source": [
    "This approach is, in fact, an approximation used to determine the most likely word $W_t$, given the words $W_{t-1}, W_{t-2}, \\ldots, W_1$ in the sequence.\n",
    "\n",
    "$P(W_t | W_{t-1}, W_{t-2}, \\ldots, W_1) \\approx P(W_t | W_{t-1})$\n",
    "\n",
    "The conditional probability $P(W_t | W_{t-1})$ signifies the likelihood of encountering the word $W_t$, based on the context provided by the preceding word $W_{t-1}$. By employing this approximation, simplify the modeling process by assuming that the occurrence of the current word is mainly influenced by the most recent word in the sequence. In general, you have the capability to identify the most likely word.\n",
    "\n",
    "$\\hat{W_t} = \\arg\\max_{W_t} \\left( P(W_t | W_{t-1}) \\right)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c6feda-8519-41fc-8f61-2f19dab53832",
   "metadata": {},
   "source": [
    "```bigrams``` is a function provided by the Natural Language Toolkit (NLTK) library in Python. This function takes a sequence of tokens as input and returns an iterator over consecutive pairs of tokens, forming bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0a2088-2cf0-4abf-87c1-2d25dc51c65f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bigrams = nltk.bigrams(tokens)\n",
    "bigrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6588b0c7-bece-4af7-bb9c-f9a39f6ce07b",
   "metadata": {},
   "source": [
    "Convert a generator into a list, where each element of the list is a bigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d23a4dc-ffd7-4b7e-ab79-2f94b7656605",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "generator"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "189ff526-9deb-4325-a558-9239746f06da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_bigrams = list(nltk.bigrams(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e27b06-9f36-4d35-aeee-4cf1cc299e2d",
   "metadata": {},
   "source": [
    "You can see the first 10 biagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f9e6829-29b7-44ee-8042-bb30025ac016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('we', 'are'),\n",
       " ('are', 'no'),\n",
       " ('no', 'strangers'),\n",
       " ('strangers', 'to'),\n",
       " ('to', 'love'),\n",
       " ('love', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'the'),\n",
       " ('the', 'rules'),\n",
       " ('rules', 'and')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_bigrams[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a002b-daef-4a63-ac6c-7243192c6ffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "Compute the frequency distribution of the bigram $C(w_{t},w_{t-1})$ using the NLTK function```bigrams```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03968ff4-ebac-4d54-91ed-8db23b3f53bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "freq_bigrams = nltk.FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f35bcf2d-9480-46b0-882a-c1e07b8927b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('gon', 'na'): 38, ('never', 'gon'): 36, ('you', 'never'): 9, ('na', 'tell'): 8, ('make', 'you'): 8, ('na', 'give'): 6, ('give', 'you'): 6, ('you', 'up'): 6, ('up', 'never'): 6, ('na', 'let'): 6, ...})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13a37e95-2a8e-43c7-b9ec-e54ddb0e72ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams[('you' , 'up')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc4b22c9-b866-476d-ad69-8e6719efebd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(freq_bigrams.keys())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aca91911-7666-4eb7-9c00-1f94846ae170",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('we', 'are')\n",
      "1\n",
      "('are', 'no')\n",
      "1\n",
      "('no', 'strangers')\n",
      "1\n",
      "('strangers', 'to')\n",
      "1\n",
      "('to', 'love')\n",
      "1\n",
      "('love', 'you')\n",
      "1\n",
      "('you', 'know')\n",
      "1\n",
      "('know', 'the')\n",
      "3\n",
      "('the', 'rules')\n",
      "1\n",
      "('rules', 'and')\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for my_bigram in my_bigrams[0:10]:\n",
    "    print(my_bigram)\n",
    "    print(freq_bigrams[my_bigram])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eff07dd2-44fc-4dc8-836c-54882f633282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word = \"strangers\"\n",
    "vocab_probs = {}\n",
    "for next_word in vocabulary:\n",
    "    vocab_probs[next_word]=freq_bigrams[(word , next_word)]/fdist[word]\n",
    "    \n",
    "\n",
    "vocab_probs=sorted(vocab_probs.items(), key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a8edf99-59d0-4e66-824c-b3a20dd30549",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to', 1.0),\n",
       " ('any', 0.0),\n",
       " ('on', 0.0),\n",
       " ('love', 0.0),\n",
       " ('so', 0.0),\n",
       " ('and', 0.0),\n",
       " ('up', 0.0),\n",
       " ('we', 0.0),\n",
       " ('each', 0.0),\n",
       " ('a', 0.0),\n",
       " ('ask', 0.0),\n",
       " ('shy', 0.0),\n",
       " ('give', 0.0),\n",
       " ('inside', 0.0),\n",
       " ('been', 0.0),\n",
       " ('if', 0.0),\n",
       " ('gon', 0.0),\n",
       " ('weve', 0.0),\n",
       " ('tell', 0.0),\n",
       " ('commitments', 0.0),\n",
       " ('whats', 0.0),\n",
       " ('never', 0.0),\n",
       " ('youre', 0.0),\n",
       " ('na', 0.0),\n",
       " ('this', 0.0),\n",
       " ('thinking', 0.0),\n",
       " ('wan', 0.0),\n",
       " ('known', 0.0),\n",
       " ('how', 0.0),\n",
       " ('hearts', 0.0),\n",
       " ('other', 0.0),\n",
       " ('it', 0.0),\n",
       " ('just', 0.0),\n",
       " ('let', 0.0),\n",
       " ('me', 0.0),\n",
       " ('know', 0.0),\n",
       " ('of', 0.0),\n",
       " ('full', 0.0),\n",
       " ('i', 0.0),\n",
       " ('are', 0.0),\n",
       " ('understand', 0.0),\n",
       " ('strangers', 0.0),\n",
       " ('dont', 0.0),\n",
       " ('around', 0.0),\n",
       " ('get', 0.0),\n",
       " ('both', 0.0),\n",
       " ('play', 0.0),\n",
       " ('lie', 0.0),\n",
       " ('what', 0.0),\n",
       " ('guy', 0.0),\n",
       " ('do', 0.0),\n",
       " ('blind', 0.0),\n",
       " ('hurt', 0.0),\n",
       " ('the', 0.0),\n",
       " ('long', 0.0),\n",
       " ('no', 0.0),\n",
       " ('run', 0.0),\n",
       " ('from', 0.0),\n",
       " ('you', 0.0),\n",
       " ('got', 0.0),\n",
       " ('desert', 0.0),\n",
       " ('too', 0.0),\n",
       " ('aching', 0.0),\n",
       " ('down', 0.0),\n",
       " ('say', 0.0),\n",
       " ('im', 0.0),\n",
       " ('make', 0.0),\n",
       " ('were', 0.0),\n",
       " ('but', 0.0),\n",
       " ('see', 0.0),\n",
       " ('game', 0.0),\n",
       " ('ta', 0.0),\n",
       " ('cry', 0.0),\n",
       " ('goodbye', 0.0),\n",
       " ('wouldnt', 0.0),\n",
       " ('rules', 0.0),\n",
       " ('feeling', 0.0),\n",
       " ('going', 0.0),\n",
       " ('for', 0.0),\n",
       " ('your', 0.0)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7dbb1432-2aa7-40db-9ced-da0d3b7277de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_probs[0:4]\n",
    "vocab_probabilities = vocab_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cc462-0474-4e39-a51b-c19819c27a1c",
   "metadata": {},
   "source": [
    "Create a function to calculate the conditional probability of $W_t$ given $W_{t-1}$, sort the results, and output them as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b21fc546-ee66-4a33-adee-e7f736757ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(my_words , freq_grams , normalize=1 , vocabulary = vocabulary):\n",
    "    vocab_probabilities = {}\n",
    "    context_size = len(list(freq_grams.keys())[0])\n",
    "    my_tokens = preprocess(my_words)[0:context_size-1]\n",
    "    \n",
    "    for next_word in vocabulary:\n",
    "        temp = my_tokens.copy()\n",
    "        temp.append(next_word)\n",
    "        \n",
    "        if normalize!=0:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)]/normalize\n",
    "        else:\n",
    "            vocab_probabilities[next_word] = freq_grams[tuple(temp)]\n",
    "    vocab_probabilities = sorted(vocab_probabilities.items() , key = lambda x : x[1], reverse = True)\n",
    "    return vocab_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ad8bfb4-1639-4a2e-8551-ca078abdcee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "<FreqDist with 80 samples and 385 outcomes>\n"
     ]
    }
   ],
   "source": [
    "y = fdist[\"i\"]\n",
    "print(y)\n",
    "print(fdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fd9e8352-96d9-40e6-8c39-7380c4d5d673",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_words = \"are\"\n",
    "vocab_probabilities = make_predictions(my_words , freq_bigrams , normalize = fdist[\"i\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d059b84-a933-4c80-abd3-32e997e29d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('no', 0.3333333333333333),\n",
       " ('any', 0.0),\n",
       " ('on', 0.0),\n",
       " ('love', 0.0),\n",
       " ('so', 0.0)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probabilities[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8abd488-5c8d-4103-81d7-36b1fffe4409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_probabilities[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a90f48-bea5-4f56-9ef7-23234cfe6d8f",
   "metadata": {},
   "source": [
    "Generate a sequence using the bigram model by leveraging the preceding word (t-1) to predict and generate the subsequent word in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a6e14876-e504-45f7-83d2-0a7a67f5fef7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('no', 0.3333333333333333)\n"
     ]
    }
   ],
   "source": [
    "max_prob = max(vocab_probabilities , key=lambda x : x[1])\n",
    "print(max_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "af8398ca-eae7-4fb2-bde5-ca4f4b63389b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "instant = \"this\"\n",
    "instant_word = make_predictions(instant , freq_bigrams)[0][1]\n",
    "print(instant_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d557ab7-0adc-4fdc-9639-7b672135ec55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_song = \"\"\n",
    "\n",
    "for w in tokens[0:100]:\n",
    "    my_word = make_predictions(w , freq_bigrams)[0][0]\n",
    "    my_song += \" \"+my_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c4440f99-9fc3-4023-ae95-fe4248943c4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' know no strangers to say you never the game and hurt long i just lie commitments what im feeling of you never get this from any other for i just wan na tell a never im feeling got ta make you never never gon na tell you never never gon na tell you never never gon na tell around and hurt you never gon na tell you never never gon na tell goodbye never gon na tell a lie and hurt you never known each other for so long your hearts been aching but youre too shy to say goodbye'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_song     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0614e77-fbdf-4897-85ed-2377a1665cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "Create a sequence using the n-gram model by initiating the process with the first word in the sequence and producing an initial output. Subsequently, utilize this output as the basis for generating the next word in the sequence, i.e., you will give your model a word, then use the output to predict the next word and repeat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0ae3a1f-b764-4a96-8e06-d06a9b5c0980",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurt\n"
     ]
    }
   ],
   "source": [
    "my_song=\"i\"\n",
    "print(my_word)\n",
    "\n",
    "for i in range(100):\n",
    "    my_word=make_predictions(my_word,freq_bigrams)[0][0]\n",
    "    my_song+=\" \"+my_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "41a08166-e221-43c0-9ccc-d075099c64ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you never gon na tell a lie and hurt you\n"
     ]
    }
   ],
   "source": [
    "print(my_song)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19893171-73fe-4d55-aa44-fe437c4dbc29",
   "metadata": {},
   "source": [
    "This method may not yield optimal results; consider the following:\n",
    "\n",
    "$\\hat{W_1}=\\arg\\max{W_1} \\left( P(W_1 | W_{0}=\\text{like})\\right)$.\n",
    "\n",
    "Upon evaluation, observe that the result for $\\hat{W}_1$ includes both \"dogs\" and \"cats\" with equal likelihood.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2aeda6-bb3a-4068-bdfd-bdb710890a8e",
   "metadata": {},
   "source": [
    "## Trigram model\n",
    "For the given example sentence: 'I like dogs and I kinda like cats'\n",
    "\n",
    "$ (I, like, dogs) $\n",
    "\n",
    "$(like, dogs, and) $\n",
    "\n",
    "$(dogs, and, I)$\n",
    "\n",
    "$(and, I, kinda)$\n",
    "\n",
    "$(I, kinda, like)$\n",
    "\n",
    "$(kinda, like, cats)$\n",
    "\n",
    "Trigram models incorporate conditional probability as well. The probability of a word depends on the two preceding words. The conditional probability $P(W_t | W_{t-2}, W_{t-1})$ is utilized to predict the likelihood of word $W_t$ following the two previous words in a sequence. The context is $W_{t-2}, W_{t-1}$ and is of length 2. Let's compute the conditional probability for each trigram:\n",
    "\n",
    "Calculate the trigram frequencies for each trigram: $Count(W_{t-2}, W_{t-1}, W_t)$\n",
    "\n",
    "### Trigram frequency counts\n",
    "\n",
    "$ \\text{Count(I, like, dogs)} = 1 $\n",
    "\n",
    "$ \\text{Count(like, dogs, and)} = 1 $\n",
    "\n",
    "$\\text{Count(dogs, and, I)} = 1$\n",
    "\n",
    "$ \\text{Count(and, I, kinda)} = 1$\n",
    "\n",
    "$ \\text{Count(I, kinda, like)} = 1 $\n",
    "\n",
    "$ \\text{Count(kinda, like, cats)} = 1 $\n",
    "\n",
    "The conditional probability $ P(w_{t} | w_{t-1}, w_{t-2})$ where $w_{t-1}$ and $w_{t-2}$ form the context, and the context size is 2.\n",
    "\n",
    "To better understand how this outperforms the bigram model, let's compute the conditional probabilities with the context \"I like\":\n",
    "\n",
    "$\\hat{W_2}=\\arg\\max{W_2} \\left( P(W_2 | W_{1}=like,W_{0}=I)\\right)$\n",
    "\n",
    "and for the words \"cats\" and \"dogs\":\n",
    "\n",
    "$ P(\"dogs\" | \"like\", \"I\") = \\frac{Count(I, like, dogs)}{Total \\ count \\ of \\ \"I\", \"like\"} = \\frac{1}{1} = 1 $\n",
    "\n",
    "$ P(\"cats\" | \"like\", \"I\") = \\frac{Count(I, like, cats)}{Total \\ count \\ of \\ \"I\", \"like\"} = 0$\n",
    "\n",
    "These probabilities signify the likelihood of encountering the third word in a trigram. Notably, the result $\\hat{W}_2$ is \"dogs,\" which seems to align better with the sequence.\n",
    "\n",
    "The trigrams function is provided by the Natural Language Toolkit (NLTK) library in Python. This function takes a sequence of tokens as input, returns an iterator over consecutive token triplets, generating trigrams, and converts them into a frequency distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "47ea935e-047c-4204-8c3c-ce8f92619540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object trigrams at 0x000001BFF7956EA0>\n"
     ]
    }
   ],
   "source": [
    "trigrams = nltk.trigrams(tokens)\n",
    "print(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ea94bd33-856a-4b1c-a82a-6bc94009c9cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('we', 'are', 'no')\n"
     ]
    }
   ],
   "source": [
    "my_trigram = list(trigrams)\n",
    "print(my_trigram[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e40e85b2-285c-4180-8bea-8d0f08ccd828",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('never', 'gon', 'na'): 36, ('you', 'never', 'gon'): 9, ('gon', 'na', 'give'): 6, ('na', 'give', 'you'): 6, ('give', 'you', 'up'): 6, ('you', 'up', 'never'): 6, ('up', 'never', 'gon'): 6, ('gon', 'na', 'let'): 6, ('na', 'let', 'you'): 6, ('let', 'you', 'down'): 6, ...})"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_trigrams = nltk.FreqDist(nltk.trigrams(tokens))\n",
    "freq_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "94d3e005-521c-416d-b5c5-a51b6013c9a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 36, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 9, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_trigrams.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ac0d0e23-f7bc-4cff-ae95-ea7cdb33abd1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([('we', 'are', 'no'), ('are', 'no', 'strangers'), ('no', 'strangers', 'to'), ('strangers', 'to', 'love'), ('to', 'love', 'you'), ('love', 'you', 'know'), ('you', 'know', 'the'), ('know', 'the', 'rules'), ('the', 'rules', 'and'), ('rules', 'and', 'so'), ('and', 'so', 'do'), ('so', 'do', 'i'), ('do', 'i', 'a'), ('i', 'a', 'full'), ('a', 'full', 'commitments'), ('full', 'commitments', 'what'), ('commitments', 'what', 'im'), ('what', 'im', 'thinking'), ('im', 'thinking', 'of'), ('thinking', 'of', 'you'), ('of', 'you', 'wouldnt'), ('you', 'wouldnt', 'get'), ('wouldnt', 'get', 'this'), ('get', 'this', 'from'), ('this', 'from', 'any'), ('from', 'any', 'other'), ('any', 'other', 'guy'), ('other', 'guy', 'i'), ('guy', 'i', 'just'), ('i', 'just', 'wan'), ('just', 'wan', 'na'), ('wan', 'na', 'tell'), ('na', 'tell', 'you'), ('tell', 'you', 'how'), ('you', 'how', 'im'), ('how', 'im', 'feeling'), ('im', 'feeling', 'got'), ('feeling', 'got', 'ta'), ('got', 'ta', 'make'), ('ta', 'make', 'you'), ('make', 'you', 'understand'), ('you', 'understand', 'never'), ('understand', 'never', 'gon'), ('never', 'gon', 'na'), ('gon', 'na', 'give'), ('na', 'give', 'you'), ('give', 'you', 'up'), ('you', 'up', 'never'), ('up', 'never', 'gon'), ('gon', 'na', 'let'), ('na', 'let', 'you'), ('let', 'you', 'down'), ('you', 'down', 'never'), ('down', 'never', 'gon'), ('gon', 'na', 'run'), ('na', 'run', 'around'), ('run', 'around', 'and'), ('around', 'and', 'desert'), ('and', 'desert', 'you'), ('desert', 'you', 'never'), ('you', 'never', 'gon'), ('gon', 'na', 'make'), ('na', 'make', 'you'), ('make', 'you', 'cry'), ('you', 'cry', 'never'), ('cry', 'never', 'gon'), ('gon', 'na', 'say'), ('na', 'say', 'goodbye'), ('say', 'goodbye', 'never'), ('goodbye', 'never', 'gon'), ('gon', 'na', 'tell'), ('na', 'tell', 'a'), ('tell', 'a', 'lie'), ('a', 'lie', 'and'), ('lie', 'and', 'hurt'), ('and', 'hurt', 'you'), ('hurt', 'you', 'weve'), ('you', 'weve', 'known'), ('weve', 'known', 'each'), ('known', 'each', 'other'), ('each', 'other', 'for'), ('other', 'for', 'so'), ('for', 'so', 'long'), ('so', 'long', 'your'), ('long', 'your', 'hearts'), ('your', 'hearts', 'been'), ('hearts', 'been', 'aching'), ('been', 'aching', 'but'), ('aching', 'but', 'youre'), ('but', 'youre', 'too'), ('youre', 'too', 'shy'), ('too', 'shy', 'to'), ('shy', 'to', 'say'), ('to', 'say', 'it'), ('say', 'it', 'inside'), ('it', 'inside', 'we'), ('inside', 'we', 'both'), ('we', 'both', 'know'), ('both', 'know', 'whats'), ('know', 'whats', 'been'), ('whats', 'been', 'going'), ('been', 'going', 'on'), ('going', 'on', 'we'), ('on', 'we', 'know'), ('we', 'know', 'the'), ('know', 'the', 'game'), ('the', 'game', 'and'), ('game', 'and', 'were'), ('and', 'were', 'gon'), ('were', 'gon', 'na'), ('gon', 'na', 'play'), ('na', 'play', 'it'), ('play', 'it', 'and'), ('it', 'and', 'if'), ('and', 'if', 'you'), ('if', 'you', 'ask'), ('you', 'ask', 'me'), ('ask', 'me', 'how'), ('me', 'how', 'im'), ('im', 'feeling', 'dont'), ('feeling', 'dont', 'tell'), ('dont', 'tell', 'me'), ('tell', 'me', 'youre'), ('me', 'youre', 'too'), ('youre', 'too', 'blind'), ('too', 'blind', 'to'), ('blind', 'to', 'see'), ('to', 'see', 'never'), ('see', 'never', 'gon'), ('hurt', 'you', 'never'), ('play', 'it', 'i'), ('it', 'i', 'just')])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_trigrams.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fce376a1-55bf-4629-b998-50d59e83ce51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lets make predictions\n",
    "\n",
    "my_word = \"so do\"\n",
    "\n",
    "results = make_predictions(my_word , freq_trigrams , normalize = freq_bigrams[(\"do\" , \"i\")])[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "86f81564-e92e-44a4-bcd8-f0a7f88e6e0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 1.0), ('any', 0.0), ('on', 0.0), ('love', 0.0), ('so', 0.0), ('and', 0.0), ('up', 0.0), ('we', 0.0), ('each', 0.0), ('a', 0.0), ('ask', 0.0), ('shy', 0.0), ('give', 0.0), ('inside', 0.0), ('been', 0.0)]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a6afe4ff-e814-4b99-9680-5b341ea871f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_song = \"\"\n",
    "\n",
    "w1 = tokens[0]\n",
    "for w2 in tokens[0:100]:\n",
    "    gram = w1+' '+w2\n",
    "    my_word = make_predictions(gram , freq_trigrams)[0][0]\n",
    "    my_song+=\" \"+my_word\n",
    "    w1=w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "9c7dfb11-c16c-47c9-be52-fb7756804919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' any no strangers to love you know the game and so do i a full commitments what im thinking of you wouldnt get this from any other guy i just wan na tell a how im feeling got ta make you cry never gon na give you up never gon na give you down never gon na give around and desert you never gon na give you cry never gon na give goodbye never gon na give a lie and hurt you never known each other for so long your hearts been aching but youre too shy to say it'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_song"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde2282-c5cd-4427-9978-b21f511ade71",
   "metadata": {},
   "source": [
    "There are various challenges associated with Histogram-Based Methods, some of which are quite straightforward. For instance, when considering the case of having N words in your vocabulary, a Unigram model would entail $N$ bins, while a Bigram model would result in $N^2$ bins and so forth.\n",
    "\n",
    "N-gram models also encounter limitations in terms of contextual understanding and their ability to capture intricate word relationships. For instance, let's consider the phrases `I hate dogs`, `I donâ€™t like dogs`, and **donâ€™t like** means **dislike**. Within this context, a histogram-based approach would fail to grasp the significance of the phrase **donâ€™t like** means **dislike**, thereby missing out on the essential semantic relationship it encapsulates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2148f0b5-d566-45d8-bca9-90f36dcd9813",
   "metadata": {},
   "source": [
    "## **Whatâ€™s Next?** ðŸš€  \n",
    "\n",
    "This project implemented a **statistical approach** to language modeling using **N-Grams**, demonstrating how word sequences can be used to predict and generate text. While effective, this method **lacks deep contextual understanding** and does not generalize well beyond the given dataset.  \n",
    "\n",
    "To achieve more **intelligent, dynamic, and human-like text generation**, the next step is to transition to **Deep Learning-based models**, such as **Feedforward Neural Networks (FFNN), Recurrent Neural Networks (RNNs), or Transformer-based architectures** like GPT. These models can learn **semantic relationships** and generate more coherent and meaningful text.  \n",
    "\n",
    "This marks the end of this project, but the journey into **Artificial Intelligence and NLP** continues! ðŸš€  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a094b9a-627e-4673-920c-ba2245ab8031",
   "metadata": {},
   "source": [
    "### **Author**  \n",
    "ðŸ‘¤ *Faizan Saleem Siddiqui*  \n",
    "ðŸ“Œ *NLP Enthusiast | AI Researcher | Open-Source Contributor*  \n",
    "\n",
    "If you found this helpful, feel free to â­ the repository and connect with me on:\n",
    "- [![LinkedIn](https://img.shields.io/badge/LinkedIn-Profile-blue?logo=linkedin)](https://www.linkedin.com/in/faizan-saleem-siddiqui-4411bb247/)  \n",
    "- [![GitHub](https://img.shields.io/badge/GitHub-Repository-black?logo=github)](https://github.com/FaizanSSDQ/NLP-and-LLMs-With-Python.git)  \n",
    "\n",
    "\n",
    "Thanks for following along! âœ¨  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71047990-dbb0-454e-904b-3f36d9121dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
